# Tamil â†’ English Translator (Seq2Seq LSTM)  
### With and Without Attention | PyTorch Implementation

This project contains **two complete neural machine translation models** built from scratch using **PyTorch**:

1. **Tamil â†’ English Translator (Without Attention)**  
2. **Tamil â†’ English Translator (With Attention Mechanism)**  

These models use:
- **Encoderâ€“Decoder architecture**
- **LSTM-based sequence modelling**
- **Teacher Forcing during training**
- **Greedy decoding during inference**

This project is meant for learning **NLP fundamentals**, understanding **Seq2Seq architecture**, and seeing how **attention improves translation quality**.

---

## ğŸš€ Project Features

### âœ”ï¸ **Model 1 â€” Seq2Seq Without Attention**
- Simple Encoderâ€“Decoder using LSTM  
- Decoder receives only the last hidden state  
- Works but struggles with longer sentences  
- Demonstrates limitations of classic seq2seq  

### âœ”ï¸ **Model 2 â€” Seq2Seq With Attention**
- Uses Bahdanau-style additive attention  
- Decoder attends to *every encoder timestep*  
- Significantly better translations  
- Visualizable attention weights  

---

## ğŸ“ Project Structure
```
MiniTranslatorWithNN/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ translator_without_attention.py
â”‚   â”œâ”€â”€ translator_with_attention.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  How the Models Work

### ğŸ”¹ 1. **Encoder**
- Tokenizes Tamil sentences  
- Converts them into embeddings  
- Passes them through an LSTM  
- Produces hidden + cell states  

### ğŸ”¹ 2. **Decoder**
- Takes English tokens step-by-step  
- Predicts the next English word  

### ğŸ”¹ 3. **Attention (Second Model Only)**
- Computes relevance between decoder state and all encoder outputs  
- Creates a â€œcontext vectorâ€  
- Helps decoder focus on correct parts of Tamil sentence  

---

## ğŸ“Š Training Details

- Loss: `CrossEntropyLoss`  
- Optimizer: `Adam`  
- Embedding size: 64  
- Hidden size: 128  
- Epochs: 300  
- Teacher Forcing: 50%  

Training logs print every 50 epochs.

---

## ğŸ“ Example Translations

| Tamil Input | English Output |
|-------------|----------------|
| à®¨à®©à¯à®±à®¿ | Thank you |
| à®¨à®¾à®©à¯ à®ªà®³à¯à®³à®¿à®•à¯à®•à¯à®šà¯ à®šà¯†à®²à¯à®•à®¿à®±à¯‡à®©à¯ | I am going to school |
| à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯? | How are you? |

With attention, translations become more accurate and fluent.

---

## â–¶ï¸ Run the Translator

### **Without Attention**
```bash
python translator_without_attention.py
```
## ğŸ‘¨â€ğŸ’» Author

Manikandaraja
Passionate about NLP, Deep Learning & building ML from scratch.